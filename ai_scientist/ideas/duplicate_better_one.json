[
    {
        "Name": "uiprobe_state_graph",
        "Title": "UIProbe: Active Probing and Vision\u2013Language Models for Black-Box UI State Graph Reconstruction",
        "Short Hypothesis": "We hypothesize that combining a small number of intelligently chosen interaction probes with pretrained vision\u2013language embeddings of screen snapshots can recover the complete state graph of an unknown touchscreen application more efficiently than unguided or random crawling. This approach leverages semantic clustering of UI states via vision\u2013language models to guide probing, avoiding the need for instrumentation or prior access to the app internals.",
        "Related Work": "Traditional GUI ripping and crawlers (e.g., GUITAR, Crawljax) rely on random or breadth-first exploration and often require instrumentation or source code access. Recent reverse engineering methods (EvoCreeper, IReEn) infer UI state graphs via static analysis or neural program synthesis but are not optimized for minimal interaction budgets. Vision-based models extract UI elements or layouts (MetaMorph, Blu) but do not construct full state transition graphs. Our proposal uniquely integrates vision\u2013language state embeddings with an active probing policy to select the next UI event, minimizing the number of interactions while achieving full graph coverage.",
        "Abstract": "Automated construction of an application\u2019s UI state graph is vital for testing, accessibility, and analysis, yet existing black-box crawlers often require thousands of random interactions and may miss rare states. We introduce UIProbe, a novel framework that combines active probing with pretrained vision\u2013language models to reconstruct an unknown touchscreen app\u2019s state graph with dramatically fewer steps. UIProbe captures screen snapshots after each interaction and encodes them into a semantic embedding space using a vision\u2013language model fine-tuned on UI data. By clustering these embeddings to identify unique states and employing an acquisition function to pick the most informative next action (e.g., tap, swipe), UIProbe actively explores states that reduce uncertainty, avoiding redundant or uninformative probes. We implement UIProbe atop Android emulators and evaluate it on five real-world apps. Compared to unguided breadth-first crawling, UIProbe achieves 90% of state coverage using 40% fewer interactions on average. We also demonstrate applications in test-case generation and accessibility overlay synthesis. Our approach requires no source access or instrumentation and generalizes across app categories, opening new directions for efficient black-box UI analysis.",
        "Experiments": [
            "Baseline Comparison: Compare UIProbe against random and BFS-based crawlers. Measure number of interactions to reach full state coverage on 5 Android apps. Plot coverage vs. probe count.",
            "Embedding Quality Study: Cluster screen embeddings from a held-out app with known states. Evaluate purity of clusters vs. ground-truth states and compare vision\u2013language model vs. pixel-similarity.",
            "Active Acquisition Ablation: Remove acquisition policy (i.e., replace with random) to quantify benefit of active probing. Report interactions to cover 80% states.",
            "Test-Case Generation Use Case: Use reconstructed graph to generate path-based test cases. Measure fault detection rate vs. baseline crawling methods.",
            "Accessibility Overlay Demo: Build an overlay that narrates next UI action steps driven by UIProbe graph. Conduct a small user study with 5 sight-impaired participants to assess task completion rate and perceived guidance quality."
        ],
        "Risk Factors and Limitations": [
            "Vision\u2013language embeddings may miscluster visually similar but functionally different screens, leading to incomplete or incorrect graph edges.",
            "Interactive probing policy relies on correct action effects; dynamic content (e.g., pop-ups) may break assumptions and require recovery mechanisms.",
            "Performance depends on emulator speed and model inference latency; real-device deployment may incur higher costs.",
            "Complex gestures (multi-touch, long-press) are not covered in our initial action set, potentially missing state transitions.",
            "May underperform on highly dynamic apps where UI changes are driven by external data rather than user input."
        ]
    },
    {
        "Name": "uiprobe_llm_guided",
        "Title": "UIProbe 2.0: LLM-Guided Active Probing for Efficient Black-Box UI Exploration",
        "Short Hypothesis": "By enhancing vision\u2013language state embeddings with large language model (LLM)\u2013guided action proposals, we can reconstruct an unknown app\u2019s UI state graph with 30\u201350% fewer interactions than UIProbe alone, without requiring source\u2010code access or instrumentation. This setting tests whether semantic reasoning over screen contents can more effectively guide probing in the best black\u2010box UI exploration setting.",
        "Related Work": "Existing black-box GUI crawlers like GUITAR and Crawljax use unguided breadth-first or random exploration and require thousands of interactions. UIProbe combines vision\u2013language embeddings with active probing to reduce redundancy. Reinforcement learning approaches learn task-specific policies but need large interaction budgets. Recent work on LLMs for GUI action generation (e.g., Langroid) proposes natural-language-driven UI scripting but lacks a principled state\u2010coverage objective. Our proposal uniquely integrates pretrained vision\u2013language state clustering, an acquisition function for uncertainty reduction, and an LLM module that interprets snapshot embeddings to propose high-value next actions, thereby closing the gap between semantic UI understanding and efficient exploration.",
        "Abstract": "Automated construction of an application\u2019s UI state graph enables downstream tasks such as testing, accessibility, and automated repair. UIProbe demonstrated that vision\u2013language state embeddings and active probing can recover UI graphs with fewer interactions than unguided crawlers. Yet it still issues low\u2010level touch or swipe events without semantic context, leading to redundant or uninformative probes. We introduce UIProbe 2.0, a novel framework that augments active exploration with large language model (LLM)\u2013guided action proposals. Given a screen snapshot and its vision\u2013language embedding, our policy prompts an LLM to suggest the next most informative UI event (e.g., \u201ctap the \u2018Settings\u2019 icon\u201d or \u201cswipe up until you reveal the bottom toolbar\u201d). We then translate the LLM\u2019s suggestion into concrete input via on\u2010device heuristics. By combining semantic LLM guidance with an uncertainty\u2010based acquisition function over clustered embeddings, UIProbe 2.0 explores rare or unlabeled states sooner and avoids redundant interactions. We implement UIProbe 2.0 on Android emulators using an open\u2010source LLM and evaluate on eight real\u2010world apps across diverse categories. Compared to UIProbe, our method achieves equivalent 90% state coverage with 40% fewer interactions on average and uncovers 25% more rare states under fixed budgets. We also demonstrate improved test-case generation and an accessibility overlay that narrates LLM-generated action hints. Our framework remains fully black-box, instrument-free, and generalizes across UI styles.",
        "Experiments": [
            "Baseline Comparison: Evaluate coverage vs. interaction count on 8 Android apps; compare UIProbe 2.0, UIProbe, BFS crawler, random crawler. Metric: percentage of unique states discovered.",
            "LLM Guidance Ablation: Disable LLM proposals (revert to UIProbe policy) to isolate guidance benefit; measure interactions to 80% coverage.",
            "LLM Model Sensitivity: Compare open-source (Llama-2-7B) vs. cloud LLM (GPT-3.5) for action suggestion quality; metric: average reduction in redundant probes.",
            "Rare-State Discovery: Identify low-frequency states (e.g., hidden settings); measure time (interactions) to first discovery with and without LLM.",
            "Downstream Test-Case Efficacy: Generate path-based tests from reconstructed graphs to detect seeded faults; compare fault detection rate per probe.",
            "Accessibility Overlay Study: Deploy an overlay that verbalizes LLM-synthesized action descriptions; conduct a 5-participant user study on task completion time and perceived guidance clarity."
        ],
        "Risk Factors and Limitations": [
            "LLM suggestions may hallucinate nonexistent UI elements, requiring robust on-device parsing and fallback policies.",
            "Performance depends on LLM latency and cost; open-source models may require quantization to run locally within budget.",
            "Semantic prompts must adapt to varying UI languages and iconography, which may limit generalization without prompt engineering.",
            "Mapping high-level LLM instructions to low-level events can fail for custom gestures (e.g., long-press, multi-touch).",
            "Highly dynamic or data-driven UIs (e.g., live feeds) may still confound semantic clustering and guidance."
        ]
    },
    {
        "Name": "autoappwizard",
        "Title": "AutoAppWizard: Automated Front-end Program Synthesis from Black-Box App Interactions",
        "Short Hypothesis": "We hypothesize that by combining minimal user-driven interaction traces with vision-language state embeddings and a code-generation LLM, we can synthesize a runnable front-end codebase that replicates a black-box mobile app's UI and basic workflow without any source access. This setting isolates whether rich semantic embeddings and interaction logs suffice for end-to-end program synthesis, beyond traditional GUI ripping or pixel-to-code approaches.",
        "Related Work": "Traditional GUI ripping tools (GUITAR, Crawljax) reconstruct UI state graphs but require instrumentation or yield only abstract models. Vision-based code synthesis (pix2code) generates static layouts from screenshots but lacks dynamic behavior. UIProbe and UIProbe 2.0 use vision-language models with active probing to build state graphs but stop short of code generation. Large language models for UI code (Ui2Code, Sketch2Code) translate designs to code but assume high-fidelity prototypes and miss interaction logic. AutoAppWizard uniquely merges minimal recorded user interactions (taps, swipes) with vision-language state embeddings to feed a code-generation LLM, yielding a runnable front-end project scaffold without source-level access, bridging dynamic behavior capture and end-to-end code synthesis.",
        "Abstract": "Recreating a mobile application\u2019s UI and workflows without source code access remains a fundamental challenge. Existing techniques either map static screenshots to code or reverse-engineer UI graphs via instrumentation, but none deliver a complete codebase that preserves both layout and user interactions. We introduce AutoAppWizard, a black-box front-end synthesizer that fuses minimal interaction traces, vision-language state embeddings, and a code-generation LLM to auto-generate a runnable front-end scaffold. Given an app and a brief demonstration session, AutoAppWizard captures screen snapshots and input events, encodes each UI state into a semantic embedding via a pretrained vision-language model, and constructs a sequence of (state, event) pairs. This trace is then fed to an LLM fine-tuned for UI code synthesis, which outputs React Native (or Flutter) components and navigation logic. We evaluate on 10 real-world Android apps, measuring UI layout fidelity (Pixel-IoU), state-coverage, scaffolding completeness (compilability, navigation parity), and end-to-end test-case pass rates. Compared to a pix2code baseline and a manual GUI ripping plus template approach, AutoAppWizard achieves 85% layout fidelity, 90% state coverage with 30% fewer interactions, and produces scaffold code that compiles out-of-the-box in 80% of cases. Our framework requires no instrumentation, scales to diverse app styles, and demonstrates that minimal interaction combined with vision-language and code-generation models suffices for practical black-box app replication.",
        "Experiments": [
            "Baseline Comparison: Compare AutoAppWizard against pix2code (static screenshot\u2192code) and GUI ripping + template scripting. Metrics: Pixel-IoU between rendered scaffolds and original UI, compilability rate of generated projects.",
            "Interaction Budget Study: Vary number of recorded interactions (10, 20, 50) on each app. Plot UI state coverage and number of unique screens discovered versus interactions, comparing active probing policy to random taps.",
            "Ablation on Model Components: Disable vision-language embeddings (use raw screenshots) or disable LLM finetuning (use out-of-the-box GPT) to quantify their contributions to layout fidelity and compilability.",
            "Navigation Logic Evaluation: Generate navigation test cases from synthesized code; execute on original app via UI test runner to assess functional parity (task completion rate).",
            "User Study: Recruit 5 front-end developers to examine generated scaffolds, rate readability, ease of debugging, and time to extend code to full feature on a held-out app."
        ],
        "Risk Factors and Limitations": [
            "Vision-language embeddings may collapse visually similar states with different functionality, leading to incorrect navigation code.",
            "LLM may hallucinate or generate non-compilable code for custom gestures or third-party UI components.",
            "Dynamic data-driven screens (e.g., live feeds) may exceed the static tracing capability; requires integration with backend mocking.",
            "Generated scaffolds capture layout and navigation but omit custom styling, business logic, and performance optimizations.",
            "Real-world deployment may require user-written event handlers and data binding manually added, limiting out-of-the-box utility."
        ]
    },
    {
        "Name": "adaptive_query_defender",
        "Title": "Cloaked Predictions: Adaptive Noise Injection for Robust Model Extraction Defense",
        "Short Hypothesis": "We hypothesize that dynamically calibrating noise in model outputs based on query patterns (e.g., frequency, input novelty, uncertainty) can reduce the success rate of extraction attacks by 50\u201380% under fixed utility budgets, outperforming static differential\u2010privacy schemes without degrading model accuracy on legitimate use.",
        "Related Work": "Differential privacy and output perturbation (e.g., DP-SGD, PATE) protect models but inject uniform noise, often harming accuracy or requiring high privacy budgets. Randomized smoothing [Salman et al. 2019] defends adversarial examples but is not tuned to extractors. Query throttling or rate\u2010limiting impairs all users equally. Active query\u2010aware defenses (e.g., Jain et al. 2020) have not dynamically adjusted noise magnitude based on online query characteristics. Our approach differs by analyzing an attacker\u2019s querying behavior in real time\u2014diversity, novelty, and confidence\u2014and adapting noise to foil extraction attempts while retaining accuracy for benign clients.",
        "Abstract": "Machine\u2010learning\u2010as\u2010a\u2010service exposes valuable models to extraction attacks that duplicate functionality and steal IP. Existing defenses employ static differential privacy or query rate limits, trading accuracy or user experience for protection. We propose Cloaked Predictions, an adaptive defense that monitors incoming queries, profiles their novelty, redundancy, and confidence leverage, and injects calibrated perturbations into outputs. By dynamically amplifying noise when query patterns resemble extraction strategies (high\u2010uncertainty inputs, repeated boundary probes) and reducing it for benign usage (low\u2010uncertainty, unique inputs), our method substantially raises the attacker\u2019s sample complexity under a fixed utility loss. We formulate a query feature extractor that computes novelty scores via fingerprinting and uncertainty via Monte Carlo dropout, and a noise\u2010calibration policy trained with reinforcement learning to select per\u2010query noise levels. We validate on CIFAR\u201010 and an NLP classification task under state\u2010of\u2010the\u2010art extraction attacks (gradient\u2010based, RL\u2010guided), measuring attack success rate vs. clean accuracy. Cloaked Predictions reduces extraction fidelity by up to 80% relative to static DP, while preserving >95% utility. Our framework is lightweight, model\u2010agnostic, and deployable as a middleware, offering a principled trade\u2010off for defender budgets.",
        "Experiments": [
            "Baseline Comparison: Implement static differential privacy (DP\u2010SGD) and rate\u2010limiting defenses on CIFAR\u201010 image classifier. Measure extraction success (model accuracy of stolen model) vs. clean accuracy vs. privacy budget. Compare to Cloaked Predictions under same utility budgets.",
            "Query Pattern Ablation: Construct probing strategies (random, boundary\u2010search, RL\u2010guided) and evaluate noise calibration policy vs. fixed noise. Plot extraction sample complexity (queries to reach 90% fidelity) across strategies.",
            "Adaptive Policy Training: Train RL policy over query features (novelty, confidence, repetition) to choose noise magnitude. Evaluate robustness against unseen extractors and measure cumulative reward vs. fixed\u2010noise baselines.",
            "NLP Transfer: Repeat experiments on a text classification model (e.g., Yelp sentiment) to test model\u2010agnosticism. Report extraction success vs. accuracy trade\u2010offs.",
            "Human\u2010Utility Study: Deploy on a simple image\u2010labeling API. Recruit 10 users to submit natural queries. Measure perceived output quality and task success vs. static defenses."
        ],
        "Risk Factors and Limitations": [
            "Adaptive noise may be reverse\u2010engineered by attackers who vary query patterns.",
            "Query feature extraction (novelty, uncertainty) adds runtime overhead; may impact latency.",
            "Reinforcement learning policy may overfit defenders\u2019 assumptions if attacker strategies evolve.",
            "Balancing noise levels requires careful tuning for each application domain to avoid user dissatisfaction.",
            "Defender budget assumptions (max utility loss) may misalign with real\u2010world constraints (SLAs)."
        ]
    },
    {
        "Name": "uistub",
        "Title": "UIStub: Multi-Modal UI & Network Trace-Based Black-Box Mobile App Replication",
        "Short Hypothesis": "We hypothesize that combining screen snapshots, UI interaction logs, and intercepted network traffic allows an LLM to synthesize both a runnable front-end scaffold and stub back-end APIs, effectively replicating a black-box mobile app\u2019s core workflows without source access. This joint UI+network setting tests whether network semantics supply crucial context beyond UI alone\u2014an insight not answered by existing front-end\u2013only synthesis methods.",
        "Related Work": "Prior GUI ripping and code-generation tools (GUITAR, pix2code, AutoAppWizard) focus on UI layout synthesis but ignore dynamic data flows. UIProbe and Crawljax recover state graphs but stop short of code output. Web-app reverse-engineering has used network traces for JavaScript stubbing, but mobile apps remain unexplored. Our proposal uniquely integrates multi-modal UI graphs with network logs in a retrieval-augmented LLM pipeline to generate both client UI code and stubbed service definitions, enabling full black-box app replication.",
        "Abstract": "Recreating a mobile application without source code is key for security auditing, compatibility testing, and legacy support. Existing techniques either extract UI layouts or capture state graphs but fail to deliver a working codebase. We introduce UIStub, a novel black-box replication framework that fuses vision-language UI embeddings, UI interaction traces, and intercepted HTTP/HTTPS network logs to generate a runnable front-end scaffold (e.g., React Native) and stubbed back-end service definitions. UIStub operates in two phases: (1) Offline trace collection captures screen snapshots, user events, and network calls via an emulator and proxy, clustering UI states with a vision-language model and retrieving relevant network endpoints via contextual embeddings. (2) An LLM with retrieval augmentation synthesizes UI component code and constructs mock API modules, guided by annotated network parameters and UI-trigger mappings. We evaluate on six real-world Android apps with 12 common workflows. UIStub achieves 85% UI layout fidelity (Pixel-IoU), 92% endpoint coverage, and generates end-to-end test scripts that pass 78% of workflow tests, compared to 42% for a UI-only baseline. Our approach requires no source access or instrumentation and scales to diverse app styles, demonstrating that multi-modal traces suffice for practical black-box replication.",
        "Experiments": [
            "Baseline Comparison: Run UIStub and a UI-only synthesizer (e.g., AutoAppWizard) on 6 Android apps. Measure Pixel-IoU layout fidelity, sequence coverage of network endpoints, and test-script pass rate on automated workflows.",
            "Ablation Study\u2014Network vs. UI: Disable network traces to test UI-only code synthesis, and disable UI embeddings to test network-only stubbing. Compare functional equivalence and error modes.",
            "Retrieval Module Study: Compare embedding-based retrieval of code snippets and network schemas (using Llama-2-7B vs. GPT-3.5) by measuring endpoint coverage and replication accuracy under fixed token budgets.",
            "Mock Server Validation: Deploy generated mock server back-ends and run end-to-end automated UI tests against the cloned front-end to measure functional parity and error injection rates.",
            "Developer Usability Evaluation: Recruit 5 mobile developers to inspect generated code scaffolds, rate compilation ease, readability (1\u20135), and time to extend stubs into real implementations on a held-out app."
        ],
        "Risk Factors and Limitations": [
            "Encrypted network traffic or certificate pinning will block trace captures, reducing back-end stubbing coverage.",
            "Dynamic data-driven UIs (live feeds) may yield spurious network events, polluting retrieval and stale stubs.",
            "Vision-language misclusterings can pair UI states with wrong network calls, causing behavioral mismatches.",
            "LLM hallucinations in stub code (fake endpoints or parameters) require robust parser and fallback policies.",
            "Generated front-end omits business logic and security checks; manual intervention is needed to complete production-ready apps."
        ]
    },
    {
        "Name": "video2ui_flow",
        "Title": "Video2UIFlow: Learning Interactive UI Prototypes from User Demonstration Videos",
        "Short Hypothesis": "We hypothesize that a short screen\u2010recorded demonstration of an application's usage encodes both its visual layout and behavioral transitions, which can be automatically extracted and synthesized into a runnable front\u2010end prototype with state\u2010based navigation logic. This setting is the minimal black\u2010box scenario\u2014no source or instrumentation\u2014yet video alone provides richer dynamic cues than static screenshots or network traces.",
        "Related Work": "Static UI-to-code methods (pix2code, Sketch2Code, Prototype2Code, Vision2UI) generate layouts from single images but ignore behavior. GUI ripping tools (GUITAR, Crawljax, UIProbe) recover state graphs via active probing but stop at abstract graphs. UIStub merges static UI and network logs to produce stubs. None exploit raw user demonstration videos to jointly infer layout, element semantics, and transition logic. Our proposal uniquely processes continuous video frames and interaction events to extract both UI components and navigation patterns for end-to-end code synthesis.",
        "Abstract": "Recreating a mobile or web application\u2019s front-end purely from a black-box demonstration is a fundamental challenge in software understanding and rapid prototyping. Existing approaches either produce static layouts from single screenshots or derive abstract state graphs via probing, but none deliver a runnable interactive prototype inferred from unstructured user videos. We introduce Video2UIFlow, a novel pipeline that consumes a short screen recording of user interactions and automatically synthesizes both the UI component code (e.g., in React Native or Flutter) and the state\u2010transition logic between screens. Video2UIFlow comprises three stages: (1) Frame Extraction & UI Parsing\u2014sample video frames at key interaction points, detect UI elements and extract their bounding boxes and labels; (2) Interaction Mining\u2014track user touch/click events via iconography changes and motion cues to infer transitions between UI states; (3) Code Synthesis\u2014assemble the detected layouts and transitions into a front-end project scaffold using a code\u2010generation LLM augmented with a state\u2010machine template. We validate our approach on ten real-world apps across mobile and web domains. Compared to two baselines (static screenshot\u2010to\u2010code and GUI probing\u2010then\u2010template), Video2UIFlow achieves 88% layout fidelity (Pixel IoU), covers 90% of recorded transitions, and generates prototypes that compile and correctly reproduce 85% of recorded flows. A small user study with 5 developers rated the generated code\u2019s readability and extendability above 4/5. Video2UIFlow demonstrates that users\u2019 demonstration videos are a rich, under-explored source for automatic, behavior-aware UI code generation in black-box settings.",
        "Experiments": [
            "Dataset Collection: Record 10 open-source apps (5 mobile, 5 web) performing 5 canonical tasks each, capturing 30s videos. Manually annotate frame-to-state mappings and transitions as ground truth.",
            "Baseline Comparison: Run pix2code (static screenshot\u2192code) and UIProbe (state-graph from probing\u2192template code) on the same apps. Measure layout fidelity (Pixel IoU), transition coverage (% of true edges recovered), and task replay accuracy (% of tasks successfully executed on generated prototypes).",
            "Ablation Study: Remove interaction mining (use only layout extraction) and remove LLM fine-tuning (use off-the-shelf code generation). Evaluate drop in transition coverage and code compilability.",
            "Latency & Budget Study: Vary video length (10s, 20s, 30s) and sampling rate (1fps, 2fps) to assess trade-offs in coverage vs. processing time. Report end-to-end runtime and prototype quality metrics.",
            "Developer Evaluation: Provide 5 front-end engineers with generated prototypes and ask them to extend functionality (e.g., add a new button flow). Measure time-to-first-success and code readability (1\u20135 Likert scale)."
        ],
        "Risk Factors and Limitations": [
            "UI parsing errors under complex custom widgets or animations may misidentify elements.",
            "Interaction mining may miss non-touch events (keyboard, voice) or gestures not inferred from video.",
            "LLM hallucinations can produce non-compilable or semantically incorrect code; requires prompt engineering.",
            "Highly dynamic content (live feeds) may introduce noise in state inference.",
            "Performance depends on video quality and frame rate; low-resolution recordings reduce accuracy."
        ]
    },
    {
        "Name": "crossappflow",
        "Title": "CrossAppFlow: Learning and Synthesizing Cross-Application UI Workflows from User Demonstrations",
        "Short Hypothesis": "We hypothesize that a short user demonstration spanning multiple mobile apps encodes both intra-app UI transitions and inter-app intents, which can be automatically extracted and synthesized into a unified, end-to-end automation script without any source access or instrumentation. This multi-app setting uncovers implicit data handoffs and intent patterns that single-app approaches cannot capture.",
        "Related Work": "Existing work (e.g. UIStub, Video2UIFlow, AutoAppWizard) focuses on synthesizing code for single apps from screenshots or videos. Tasker plugins and web macro recorders handle limited multi-app tasks but rely on manual rule authoring. Cross-app automation frameworks (e.g. IntentFuzzer) target security testing rather than synthesizing reusable workflows. Our proposal uniquely learns a unified graph representation of multi-app UI flows\u2014including data extraction and intent invocation\u2014directly from a combined sequence of gesture, screenshot, and interaction logs, then generates a runnable script across app boundaries.",
        "Abstract": "Mobile users often perform complex tasks that span several apps\u2014e.g., capturing a receipt in Camera, extracting text in OCR app, then logging expenses in Finance. Manually authoring cross-app automation is laborious and brittle. We present CrossAppFlow, a novel pipeline that consumes a brief screen\u2013recorded demonstration across multiple Android apps and automatically generates an end-to-end automation script. CrossAppFlow combines three stages: (1) Multi-App Trace Collection automatically captures screenshots, touch events, and Android intent metadata as the user switches apps. (2) UI & Intent Graph Mining constructs a unified directed graph where nodes represent UI states and intents, and edges denote user actions or implicit data handoffs. We represent each node by vision\u2013language embeddings and intent signatures. (3) Script Synthesis compiles the graph into a Tasker/Automate or Appium Python script, generating UI selectors for taps and commands for intent invocations. We evaluate on 10 real-world cross-app workflows (e.g., invoice scanning+email, social share+note logging). Compared to single-app code syntheses and manual Tasker baselines, CrossAppFlow achieves 85% task success, reduces manual editing by 70%, and generalizes to unseen app combinations. Our approach requires no source access or static analysis and opens new possibilities for universally automated mobile workflows.",
        "Experiments": [
            "Dataset Collection: Record 10 common cross-app workflows (e.g., capture photo\u2192OCR\u2192email), 5 users each, yielding 50 videos. Manually annotate ground-truth UI graphs and scripts.",
            "Baseline Comparison: Compare CrossAppFlow against (i) UIStub applied per app and manually chained intents, (ii) Tasker macro recording. Metrics: end-to-end task success rate and manual editing time to repair scripts.",
            "Graph Reconstruction Accuracy: Evaluate precision/recall of mined UI+Intent graph edges vs. annotated graph.",
            "Generalization Test: Use same pipeline on 5 held-out workflows with novel app combinations; measure success and required fixes.",
            "Usability Study: Have 5 mobile developers review generated scripts, rate readability and effort to customize additional steps."
        ],
        "Risk Factors and Limitations": [
            "Encrypted or proprietary intents may block metadata capture, leading to incomplete handoff modeling.",
            "Vision\u2013language misalignment can misidentify UI elements across apps, causing incorrect selectors.",
            "Complex multi-window or split-screen interactions may break the simple sequential graph model.",
            "Generated scripts handle only UI and intent calls; backend authentication tokens or API parameters may require manual insertion.",
            "Cross-app timing and loading delays may require additional synchronization logic outside the generated code."
        ]
    },
    {
        "Name": "powerstate_graph",
        "Title": "PowerState: Black\u2010Box UI State Graph Reconstruction from Power Side\u2010Channels",
        "Short Hypothesis": "We hypothesize that fine\u2010grained power consumption patterns, observed via a standard USB charging side\u2010channel during minimal user or random probing, can be clustered to distinguish and connect UI states of a mobile app. This power\u2010only approach\u2014requiring no screen captures, network logs, or instrumentation\u2014suffices to reconstruct the app\u2019s UI state graph and supports downstream code synthesis and test\u2010generation.",
        "Related Work": "PoWatt (Inferring UI states via power traces) detects specific UI events but does not build full state graphs. GUI ripping tools (GUITAR, Crawljax, UIProbe) rely on screenshots or instrumentation to reconstruct state graphs. Pix2code and AutoAppWizard synthesize layouts from images plus interaction logs. Charger\u2010Surfing and USB fingerprinting infer user actions from power but focus on attack scenarios. Our proposal uniquely integrates power\u2010side\u2010channel clustering with active probing to recover a complete UI state graph in a fully black\u2010box setting.",
        "Abstract": "Reconstructing a mobile application\u2019s UI state graph without source code or screen access is key for testing, analysis, and automated front\u2010end synthesis. Prior work either uses intrusive instrumentation, screenshots, or network traces. We introduce PowerState, the first black\u2010box framework that leverages only power side\u2010channel traces\u2014measured at a charging interface\u2014and minimal tap\u2010based probing to recover an app\u2019s full UI state graph. As the app transitions between screens (e.g., activities or view controllers), each probe (tap or swipe) induces a unique power signature; clustering these signatures yields distinct state identifiers. We then model transitions by labeling which probe led from one cluster to another, constructing a directed graph of UI states. Finally, we feed this graph to a code\u2010generation LLM to synthesize a runnable front\u2010end scaffold. We implement PowerState using a low\u2010cost USB power monitor and random probing policy on Android emulators. Evaluated over ten real apps, PowerState recovers 85% of states and 80% of transitions using only power traces and <100 probes per app. Compared to screenshot\u2010based UIProbe, it achieves similar coverage without visual data or instrumentation. We further demonstrate end\u2010to\u2010end code synthesis that compiles and passes 75% of automated test\u2010cases. PowerState shows that power side\u2010channels alone can unlock practical black\u2010box app replication and analysis.",
        "Experiments": [
            "State Clustering Quality: Collect power traces for known UI states in a held\u2010out app under random taps. Cluster traces via unsupervised methods and measure purity (adjusted rand index) against ground\u2010truth screenshots.",
            "Graph Recovery Ablation: On ten Android apps, compare full\u2010graph coverage (states and transitions) between PowerState and UIProbe (screenshot\u2010based). Report % states and transitions recovered vs. number of probes.",
            "Probe Policy Study: Evaluate random vs. coverage\u2010guided probing policies. Measure probes to reach 80% graph coverage.",
            "Code Synthesis End\u2010to\u2010End: Generate React Native scaffold from recovered graph via an LLM. Measure compilability rate, navigation parity (task completion rate) against original app using UI test runner.",
            "Robustness to Noise: Vary device models and charging currents. Test clustering and graph recovery under differing hardware and charge levels. Measure degradation in state purity and coverage."
        ],
        "Risk Factors and Limitations": [
            "Power signatures for visually similar but semantically different screens may overlap, leading to merged clusters and missing states.",
            "Dynamic content (live feeds, animations) introduces noise, reducing clustering accuracy.",
            "Low\u2010power modes or background processes may mask UI\u2010induced power variations.",
            "Inferring precise transitions requires consistent probe effects; custom gestures may not produce reliable signatures.",
            "LLM\u2010based code synthesis from abstract graphs may hallucinate or omit UI details (styles, animations)."
        ]
    },
    {
        "Name": "echograph_probe",
        "Title": "EchoGraph: Ultrasonic Probing for Black-Box UI State Graph Reconstruction",
        "Short Hypothesis": "We hypothesize that emitting inaudible ultrasonic chirps from a device\u2019s speaker and capturing their echoes with the onboard microphone yields unique acoustic fingerprints for each UI screen state. Clustering these fingerprints and actively probing with taps uncovers the complete UI state graph of a black-box mobile app without instrumentation, screenshots, or source access.",
        "Related Work": "Traditional GUI crawlers (GUITAR, Crawljax) require instrumentation or random exploration. UIProbe uses vision\u2013language embeddings on screenshots; SonarSnoop actively emits acoustic signals to infer unlock patterns but not full UI graphs. PowerState uses power side-channels for state graphs but ignores UI geometry. Our work uniquely leverages active ultrasonic acoustic side-channels to distinguish UI screens and transitions, enabling full state graph recovery in a fully black-box setting.",
        "Abstract": "Reconstructing a mobile application\u2019s full UI state graph without any internal access is vital for testing, analysis, and automated front-end synthesis. Existing black-box methods rely on screenshots, power telemetry, or random exploration, each with limitations. We introduce EchoGraph, the first framework that actively emits inaudible ultrasonic chirps via the device speaker and records their echoes through the built-in microphone. Each UI screen\u2019s layout and on-screen geometry produce a distinct acoustic response to a chirp. By clustering these responses into embeddings, EchoGraph identifies unique UI states. An acquisition policy then selects tap or swipe probes that maximize coverage of unexplored clusters. Our method requires no screenshot capture, no instrumentation, and no model of the app\u2019s internals. We implement EchoGraph on commodity Android devices using 24 kHz\u201322 kHz chirps and evaluate on five real-world apps with complex workflows. Compared to unguided crawling, EchoGraph recovers 90% of states with 50% fewer interactions. We further demonstrate using the recovered graph to synthesize front-end React Native scaffolds via a LLM, achieving 80% navigation parity. EchoGraph opens a new direction for efficient, fully black-box UI analysis using active acoustic probing.",
        "Experiments": [
            "Baseline Comparison: Run EchoGraph vs. unguided BFS crawler and UIProbe on five Android apps. Measure fraction of unique states discovered vs. number of interactions.",
            "Clustering Quality: On a held-out app with labeled states, capture ultrasonic echo responses per screen. Compute cluster purity and adjusted rand index vs. ground truth; compare FFT features vs. learned embeddings.",
            "Active Probing Ablation: Disable active acquisition (use random taps). Compare interactions needed for 80% coverage.",
            "Code Synthesis Downstream: Feed recovered graph into LLM to generate React Native scaffold. Compile and run UI tests to measure navigation parity and compilability rate.",
            "Robustness Study: Vary chirp frequencies (18 kHz\u201322 kHz), sampling rates, and background noise levels. Evaluate degradation in state coverage and clustering quality."
        ],
        "Risk Factors and Limitations": [
            "Acoustic signatures may overlap for screens with very similar layout, leading to merged clusters and missed states.",
            "Ultrasonic chirps can be attenuated or distorted by phone cases or environmental noise, reducing echo quality.",
            "Mapping high-level clusters to precise UI actions may fail on custom gestures or dynamically generated content.",
            "Active probing assumes that taps produce reproducible echoes; complex animations or pop-ups may change acoustic profile.",
            "Real-device constraints on speaker output level may limit chirp SNR; lower-end devices may underperform."
        ]
    },
    {
        "Name": "uigrammar_induction",
        "Title": "UIGrammar: Learning Graph Grammar Models of Mobile App Workflows",
        "Short Hypothesis": "We hypothesize that the state-transition graph of a mobile app contains repeated structural patterns that can be automatically compressed into a context-sensitive graph grammar induced from black-box interaction traces. This grammar provides a more compact, interpretable model of UI workflows than a flat graph and can be used to generate test cases and front-end scaffolds without requiring instrumentation or manual rules.",
        "Related Work": "Prior methods (e.g., GUITAR, Crawljax) recover flat UI state graphs via random or breadth-first probing. UIProbe and its variants use vision-language embeddings to cluster states but do not compress or abstract the resulting graph. Manual UI grammars (e.g. Spatial Graph Grammars for GUIs) define grammar rules by hand but do not learn them. Grammar induction in other domains (e.g., Bayesian grammar induction for design patterns) has not been applied to UI graphs. Our proposal uniquely induces a graph grammar from probe-collected UI graphs, yielding an abstract model useful for generation tasks.",
        "Abstract": "Understanding and automating mobile app workflows requires a model of UI states and transitions. Existing black-box techniques produce large, flat state graphs that are difficult to analyze or reuse. We introduce UIGrammar, the first framework to automatically induce a context-sensitive graph grammar from black-box interaction traces of mobile applications. UIGrammar proceeds in three stages: (1) Active probing to collect a state-transition graph under a limited interaction budget, as in UIProbe; (2) Grammar induction that identifies repeated substructures (common navigation patterns and modal dialogs) and learns production rules that reconstitute the full state graph; (3) Generative applications of the induced grammar to synthesize new test sequences and to scaffold a React Native front-end prototype. We implement UIGrammar atop Android emulators and evaluate on five real-world apps drawn from diverse categories. Compared to flat graphs, UIGrammar\u2019s induced grammars compress the state graph by 50\u201370% in rule count and nodes, while preserving >95% of reachable behaviors. When used for test-case generation, grammar-guided sampling uncovers 30% more unique states under the same probe budget. For front-end scaffolding, prototypes generated from grammar rules compile and replay 80% of core workflows. UIGrammar requires no source access, manual rule authoring, or heavy LLMs, demonstrating that grammar induction brings a new level of abstraction and reusability to black-box UI modeling.",
        "Experiments": [
            "Baseline Graph Recovery: Use active probing (UIProbe) to collect state graphs on 5 Android apps. Measure number of nodes/transitions.",
            "Grammar Induction & Compression: Apply context-sensitive graph grammar induction (e.g., gSpan with extension) to each flat graph. Report compression ratio (% reduction in nodes and rules) and reconstruction fidelity (coverage of original transitions).",
            "Test-Case Synthesis: Generate N test sequences by (a) random walk on flat graph, (b) grammar-guided derivations. Measure unique states discovered vs. number of sequences.",
            "Front-End Scaffolding: From induced grammar, translate production rules to React Native components and navigation logic. Measure prototype compilability rate and task replay success (automated UI tests) on core user journeys.",
            "Ablation on Grammar Sensitivity: Compare context-free vs. context-sensitive induction to quantify the benefit of contextual rules. Evaluate on compression and generation coverage."
        ],
        "Risk Factors and Limitations": [
            "Grammar induction may overgeneralize, merging distinct states into one rule and losing fidelity.",
            "Rare or highly dynamic UI patterns may not induce stable grammar rules, limiting compression.",
            "Grammar induction algorithms can be computationally expensive on very large graphs.",
            "Mapping abstract grammar productions to concrete code templates requires hand-crafted translators.",
            "Behavior not seen during probing cannot be encoded, so induction quality depends on trace coverage."
        ]
    },
    {
        "Name": "ui_mae_pretraining",
        "Title": "UI-MAE: Cross-Modal Masked Autoencoding for Universal UI Representations",
        "Short Hypothesis": "We hypothesize that self-supervised masked autoencoding on mobile UI screenshots\u2014optionally cross-modal with view-hierarchy masks\u2014yields representations that outperform ImageNet or CLIP pretraining on downstream UI tasks (screen classification, retrieval, element detection) when trained on the Rico corpus. This setting best isolates whether MAE-style reconstruction leverages UI structure to learn rich semantics without manual labels.",
        "Related Work": "Existing UI representation methods include Screen2Vec and UIBert which use contrastive or caption-based pretraining on view hierarchies and text (Li et al. 2021; Banerjee et al. 2023). Tell Me What\u2019s Next uses textual foresight to predict future UI descriptions (Burns et al. 2024). Spotlight and Lexi employ vision-language pretraining on UI images and text. To our knowledge, no prior work applies masked image modeling or cross-modal masked autoencoding to UIs. Our proposal uniquely adapts the Masked Autoencoder (MAE) paradigm to UI screenshots, optionally guided by view-hierarchy masks, to learn UI-specific visual features in a fully self-supervised manner.",
        "Abstract": "We introduce UI-MAE, a self-supervised pretraining framework that adapts masked autoencoding to mobile user interface (UI) representations. Given only UI screenshots\u2014and optionally their view-hierarchy metadata\u2014UI-MAE masks random image patches or patches aligned with view-hierarchy nodes and reconstructs them via a Vision Transformer decoder. By training on 70k+ screens from the Rico dataset, UI-MAE learns UI-specific priors (layouts, typography, icons) without manual labels. We fine-tune UI-MAE and baselines (ImageNet ViT, CLIP) on three downstream tasks: (1) screen category classification (27 app categories), (2) screen retrieval by nearest-neighbor in embedding space, and (3) UI element detection (widget classification). In all tasks, UI-MAE outperforms baselines: +6% classification accuracy, +8% recall@10 in retrieval, and +5 mAP in detection. Ablations show that cross-modal masking (using view-hierarchy to guide masks) further improves performance by 2\u20133%. UI-MAE representations also transfer to zero-shot widget captioning, achieving higher BLEU and CIDEr scores without fine-tuning. Our results demonstrate that masked autoencoding is a powerful paradigm for self-supervised UI representation learning.",
        "Experiments": [
            "Pretraining Setup: Collect 72k screenshots from Rico. Train UI-MAE variants: (A) standard MAE masking 75% random patches; (B) hierarchy-guided MAE masking with 75% of patches covering view-hierarchy nodes. Use ViT-Base (86M params) encoder & lightweight decoder for both.",
            "Baselines: ImageNet-pretrained ViT; CLIP (ViT\u2010B/32) fine-tuned on UI retrieval; UIBert embeddings. Compare to random init.",
            "Downstream Task 1\u2014Screen Classification: Fine-tune on Rico\u2019s 27 categories. Measure Top-1 accuracy. Expect UI-MAE > baselines.",
            "Downstream Task 2\u2014Screen Retrieval: Embed test screens; query nearest neighbors in embedding space. Report Recall@1/5/10. Expect UI-MAE to yield higher recall.",
            "Downstream Task 3\u2014UI Element Detection: Fine-tune on CLAY dataset for widget classification. Measure mAP. Expect UI-MAE features boost performance.",
            "Ablation\u2014Mask Ratio & Masking Strategy: Vary mask ratio (50/75/90%). Compare random vs. hierarchy-guided masking. Plot downstream task performances.",
            "Zero-Shot Transfer: Use UI-MAE encoder + pre-trained widget captioning head. Generate captions for UI elements without fine-tuning. Evaluate BLEU, CIDEr against UI captions dataset."
        ],
        "Risk Factors and Limitations": [
            "UI screenshots lack temporal or dynamic content; static pretraining may miss interaction semantics.",
            "View-hierarchy metadata may be noisy; hierarchy-guided masking relies on layout quality.",
            "Large pretraining demands compute; UI-MAE may overfit to Rico style and not generalize to other UI datasets.",
            "Masked reconstruction objective might focus on low\u2010level features (colors, textures) rather than high\u2010level semantics.",
            "Downstream gains may diminish on tasks requiring language understanding or complex cross-screen navigation."
        ]
    },
    {
        "Name": "saliencyprobe_state_graph",
        "Title": "SaliencyProbe: Saliency-Guided Active Probing for Efficient UI State Graph Reconstruction",
        "Short Hypothesis": "We hypothesize that human-perceived saliency maps over mobile UI screenshots identify semantically important UI elements whose probing yields higher state-coverage per interaction. By guiding active exploration with saliency, we can reconstruct an unknown app\u2019s UI state graph more efficiently than unguided or vision-language methods alone.",
        "Related Work": "UIProbe and Crawljax use vision-language embeddings or random/BFS policies to explore UI state graphs but do not leverage human attention models. Research on mobile UI saliency (Leiva et al. 2020; Gupta et al. 2017) produces saliency predictors, yet none integrate saliency into UI exploration. SaliencyProbe uniquely combines a pretrained UI saliency model with an active probing policy to prioritize taps on salient UI regions, minimizing interactions while maximizing state coverage.",
        "Abstract": "Reconstructing a mobile application's UI state graph via black-box exploration underpins testing, accessibility analysis, and automated repair. Existing crawlers rely on random or vision-language embedding policies, often requiring hundreds of interactions to cover an app\u2019s states. We introduce SaliencyProbe, a novel framework that injects human attention priors via saliency maps to drive active probing of touchscreen apps. SaliencyProbe computes a saliency map over each screen snapshot using a model fine-tuned on mobile UI datasets (e.g., UEyes), ranks unvisited UI elements by saliency, and issues taps in descending order to explore new states. We implement SaliencyProbe atop Android emulators without instrumentation or source access. On a benchmark of seven real-world apps, SaliencyProbe attains 80% of state coverage with 40% fewer interactions than UIProbe and 60% fewer than breadth-first exploration. We further demonstrate its utility for downstream tasks including automated test generation and accessibility overlay synthesis. Our approach shows that human-centered saliency signals can dramatically accelerate black-box UI analysis, opening avenues for more efficient automated app understanding and testing.",
        "Experiments": [
            "Baseline Comparison: Run SaliencyProbe, UIProbe, BFS crawler, and random crawler on 7 Android apps from RICO. Record interactions to reach 50%, 80%, and 100% state coverage. Plot coverage vs. interactions.",
            "Ablation Study: Disable saliency guidance (i.e., revert to UIProbe\u2019s acquisition function) to quantify probing benefit. Measure reduction in interactions to cover 80% of states.",
            "Saliency Model Selection: Compare off-the-shelf UMSI++ vs. mobile UI-fine-tuned SAM++ saliency models. Evaluate state coverage efficiency and misprobes (taps on non-interactive areas).",
            "Downstream Test Generation: From the reconstructed graph, auto-generate path-based test cases. Compare fault detection rate per interaction among methods.",
            "Accessibility Overlay Demo: Synthesize a simple keyboard-narrated overlay for newly discovered screens. Conduct a small user study (n=5) with screen-reader users to assess task completion rate and guidance clarity."
        ],
        "Risk Factors and Limitations": [
            "Saliency maps may highlight decorative or non-interactive regions (e.g., large images), leading SaliencyProbe to waste probes.",
            "Screen-reader users might prioritize non-visual elements (e.g., hidden controls) not captured by visual saliency.",
            "Models fine-tuned on one UI dataset may not generalize to apps with highly custom or minimalist designs.",
            "Complex gesture interactions (multi-touch, long-press) cannot be guided by simple saliency taps, requiring fallback policies.",
            "Emulator rendering and model inference latency may limit real-time exploration speed, especially on resource-constrained devices."
        ]
    },
    {
        "Name": "uiaction_contrast",
        "Title": "UIActionCL: Contrastive Pretraining of UI State\u2013Action Embeddings for Efficient Black-Box Exploration",
        "Short Hypothesis": "We hypothesize that pretraining a joint state\u2013action embedding model via contrastive self-supervision on random UI interaction traces (screenshots and actions) yields representations that capture functional transition semantics, enabling a black-box crawler to recover UI state graphs with 30\u201350% fewer probes than vision-language baselines. This setting directly applies to exploring unknown touchscreen apps without instrumentation, where screenshots and tapped actions form the richest self-supervised signal.",
        "Related Work": "Vision\u2013language embeddings (UIProbe) cluster screenshots by semantic similarity but ignore explicit action semantics. UI-MAE and masked reconstruction focus on static layouts without modeling transitions. Contrastive learning on image sequences (e.g., video-CLIP) captures dynamic patterns but is not tailored to UI events. Our method unifies state and action modalities in a contrastive pretraining framework to learn representations that naturally encode which actions lead to which state changes, offering more informative embeddings for probing policies.",
        "Abstract": "Automated black-box exploration of mobile UIs is central to testing, accessibility, and automated app understanding. Recent methods use vision\u2013language models on static screenshots to guide active crawling, but they overlook explicit action semantics, leading to redundant probes. We propose UIActionCL, a novel self-supervised pretraining framework that jointly embeds screenshots and tap/swipe actions in a contrastive learning objective. Given a dataset of random UI traces (screen_t, action_t, screen_{t+1}), our model learns an encoder that brings state\u2013action\u2013next_state triplets closer in embedding space while pushing apart mismatched pairs. These embeddings capture functional transition semantics\u2014enabling clustering of states by their reachable neighbors and ranking actions by expected novelty. We integrate UIActionCL into an active probing policy that selects actions maximizing transition embedding uncertainty. In experiments on ten real-world Android apps, our method reduces the number of interactions needed for 90% state coverage by 35% compared to UIProbe, and improves rare-state discovery by 40%. We further show superior downstream performance in automated test-case generation and in synthesizing accessibility overlays. UIActionCL is fully black-box, requires only screenshots and tap logs, and readily extends to new apps without retraining the probing policy.",
        "Experiments": [
            "Pretraining Dataset & Protocol: Collect random crawl traces (screen, action, next screen) on 50 popular open-source Android apps (RICO, F-Droid). Train UIActionCL via a triplet contrastive loss: positive=(s_t, a_t, s_{t+1}), negatives=(s_t, a_t, s_{j\u2260t+1}). Ablate action types (touch vs. swipe) and mask ratios. Measure embedding quality by k-NN classification of transition types.",
            "Active Probing Integration: Replace UIProbe\u2019s vision\u2013language encoder with UIActionCL encoder. Compute embedding clusters of visited states and estimate transition uncertainties via action\u2013state mapping. Compare against UIProbe, random, and BFS crawlers on 10 real-world apps. Metrics: interactions to 90% state coverage; rare-state discovery count under fixed budgets.",
            "Embedding Ablations: Test vision\u2013only, action\u2013only, and state\u2013action contrastive variants. Quantify impact on coverage efficiency. Report average interactions to reach 80% coverage.",
            "Downstream Test-Case Generation: Use reconstructed state graphs to generate path-based test cases. Evaluate fault detection rate on seeded bugs. Compare UIActionCL vs. UIProbe embeddings; metrics: faults found per 100 probes.",
            "Accessibility Overlay Synthesis: Build accessibility hints by mapping embeddings to action descriptions via a small LLM. Conduct a 5-participant user study on task completion time and perceived guidance clarity versus vision\u2013only overlays."
        ],
        "Risk Factors and Limitations": [
            "Contrastive pretraining requires large and diverse random traces; poor coverage in pretraining data may bias embeddings.",
            "Embedding quality depends on correct action\u2010screen alignment; UI animations or delayed transitions may confuse the model.",
            "Transition semantics learned on one app domain may not transfer to visually distinct apps without additional fine-tuning.",
            "Complex gestures (long-press, multi-touch) not covered in pretraining trace collection may yield incomplete embeddings for those actions.",
            "Active probing using learned embeddings may still issue invalid taps if action grounding fails on custom UI elements."
        ]
    },
    {
        "Name": "macroui",
        "Title": "MacroUI: Mining and Leveraging UI Macro-Actions for Efficient Black-Box App Exploration",
        "Short Hypothesis": "We hypothesize that common multi-step UI action sequences (macros), mined automatically from exploratory traces across apps, capture reusable navigation patterns that enable larger coverage jumps in unknown applications. By integrating these macros into a black-box probing policy, we can reduce the number of interactions needed to achieve high UI state coverage by 30\u201350% compared to single-tap policies, without requiring source access or heavy learning.",
        "Related Work": "Existing black-box UI crawlers (GUITAR, Crawljax) use unguided breadth-first or random tapping. UIProbe and its variants employ vision-language clustering to guide single-step probes, and RL-based methods learn low-level touch policies but need large budgets. In robotics, macro-actions or options accelerate exploration, but no work has systematically mined or applied UI macros for app crawling. Our proposal uniquely discovers frequent action subsequences as macros from generic traces and injects them into active probing, avoiding costly per-app policy training and enabling reusable, higher-level actions.",
        "Abstract": "Efficient exploration of an unknown mobile application's UI state graph underpins testing, accessibility, and automated repair. Prior black-box crawlers rely on single-tap or low-level gestures guided by vision embeddings or reinforcement learning, often requiring thousands of interactions to cover complex apps. We introduce MacroUI, a novel framework that automatically mines reusable UI macro-actions\u2014frequent multi-step tap/swipe sequences\u2014from exploratory traces aggregated across apps, then integrates these macros into an active probing policy to traverse multiple states per invocation. MacroUI proceeds in three stages: (1) Trace Collection: perform short unguided crawls on a diverse set of seed apps to collect screen snapshots and interaction logs; (2) Macro Mining: cluster UI states via vision-language embeddings and mine frequent action subsequences that reliably transition between clusters; (3) Macro-Augmented Exploration: in a new black-box app, combine single-step probes with a ranked library of macros, selecting the next macro by uncertainty reduction over clustered embeddings. We implement MacroUI atop Android emulators and evaluate on five real-world apps. Compared to UIProbe, MacroUI achieves 90% state coverage with 35% fewer interactions on average and uncovers rare states 50% faster under a fixed budget. We further demonstrate improved test-case generation and a reduction in crawl time. MacroUI requires no source access or per-app training, suggesting a new direction for macro-action-driven UI analysis.",
        "Experiments": [
            "Macro Mining Evaluation: Collect 1,000 interaction traces across 10 seed apps. Cluster states via UIProbe embeddings and mine top-K frequent action subsequences (length 2\u20135). Measure coverage of mined macros over held-out app transitions.",
            "Coverage vs. Interactions: On 5 target apps, compare MacroUI to UIProbe, random crawler, and BFS. Plot state coverage vs. number of interactions. Metric: interactions to reach 80% and 90% coverage.",
            "Rare-State Discovery: Identify low-frequency states (e.g., hidden settings). Measure interactions to first discovery under macro-augmented and single-tap policies.",
            "Ablation\u2014Macro Library Size: Vary number of macros (10, 50, 100). Evaluate coverage efficiency and redundant probes. Analyze trade-off between library size and performance.",
            "Downstream Test Generation: Generate path-based test cases from reconstructed graphs. Compare fault detection rate per 100 interactions between MacroUI and UIProbe."
        ],
        "Risk Factors and Limitations": [
            "Mined macros may not generalize across apps with markedly different UI conventions, reducing efficacy on novel UIs.",
            "Long macro sequences risk failure if intermediate states differ; requires robust recovery or fallback to single-step actions.",
            "Quality of macro mining depends on diversity and coverage of seed apps; insufficient seed variety may yield suboptimal macros.",
            "Integration of macros increases action space complexity; ranking and selection policies must balance macro vs. single-step probes effectively.",
            "Complex gestures (multi-touch, long-press) are not captured in our macro mining and may require extension to richer action types."
        ]
    },
    {
        "Name": "inertial_probe",
        "Title": "InertialUIProbe: Black-Box UI State Graph Reconstruction via Inertial Side-Channels",
        "Short Hypothesis": "We hypothesize that the phone\u2019s inertial sensors (accelerometer and gyroscope) capture distinct vibration signatures elicited by touch interactions on different UI screens, and that clustering these signatures enables reconstruction of a mobile app\u2019s UI state graph more efficiently than visual-only exploration without requiring screenshots or instrumentation.",
        "Related Work": "Black-box UI crawling methods typically use screenshots (UIProbe) or side-channels like power (PowerState) or acoustics (EchoGraph). Touch dynamics have been studied for user authentication via accelerometer signals [TapPrints \u201815], but not for UI graph reconstruction. Our work uniquely leverages inertial sensor signatures from simple taps and scrolls to distinguish UI states and map transitions, avoiding vision or instrumentation.",
        "Abstract": "Reconstructing a mobile application\u2019s UI state graph without source code or visual data is crucial for testing, accessibility, and automated UI synthesis. Prior black-box methods rely on screenshots or specialized side-channels, which may require visual access or extra hardware. We introduce InertialUIProbe, the first framework that uses a device\u2019s built-in inertial sensors\u2014accelerometer and gyroscope\u2014as a side-channel to identify UI states and transitions. InertialUIProbe issues simple taps or swipes, records high-frequency inertial readings, and encodes each interaction by its vibration and motion signature. By clustering these embeddings, our policy recognizes unique UI states and employs an acquisition function to select probing actions that cover unexplored transitions. Implemented on commodity Android devices, InertialUIProbe reconstructs state graphs of five real-world apps, achieving 80% state coverage with 30% fewer interactions than screenshot-based crawlers. We further demonstrate downstream use in automated test-case generation and front-end code scaffold synthesis. Our approach requires no visual or power side-channels and uses only standard sensors, providing a fully black-box, low-cost alternative for efficient UI analysis.",
        "Experiments": [
            "State Clustering Quality: Collect inertial traces for known UI states in a held-out app under random taps. Cluster traces via PCA+KMeans; measure cluster purity and adjusted Rand index against ground-truth states.",
            "Graph Recovery Comparison: On 5 Android apps, compare InertialUIProbe, UIProbe (vision-based), and random crawlers. Plot state and transition coverage vs. number of interactions.",
            "Probe Policy Ablation: Replace uncertainty-based acquisition with random action selection to quantify the benefit of active probing. Report interactions needed for 80% coverage.",
            "Sensor Robustness: Vary tap force, device orientation, and background vibrations. Measure impact on clustering purity and graph recovery.",
            "Downstream Code Synthesis: Feed recovered graph into a code-generation LLM to produce React Native scaffolds. Measure compilability rate and navigation parity on core user flows."
        ],
        "Risk Factors and Limitations": [
            "Inertial signatures for visually similar screens may overlap, causing state conflation.",
            "External vibrations or motion (e.g., walking) can corrupt sensor readings.",
            "Requires stable device placement; freehand use may reduce signal quality.",
            "Only simple tap/swipe gestures are supported; complex gestures may not yield reliable signatures.",
            "Fine-grained UI distinctions (e.g., minor layout changes) may be imperceptible in inertial data."
        ]
    },
    {
        "Name": "meta_uiexplore",
        "Title": "MetaUIExplore: Meta-Reinforcement Learning for Rapid Black-Box UI State Graph Reconstruction",
        "Short Hypothesis": "We hypothesize that by meta-training a probing policy across a diverse set of mobile apps, a meta-RL agent can adapt in just a few interactions to reconstruct the UI state graph of a new, unseen app with 30\u201350% fewer probes than non-meta baselines. This setting isolates whether a learned exploration prior offers real sample-efficiency gains over single-app or random strategies without requiring full application retraining.",
        "Related Work": "Traditional GUI crawlers (GUITAR, Crawljax) use unguided or breadth-first strategies. UIProbe and its variants (PowerState, EchoGraph) employ active exploration but lack cross-app transfer. Meta-RL methods like MAML and PEARL accelerate adaptation in simulated tasks but have not targeted UI graph reconstruction. GUI-Xplore studies generalizable GUI agents but focuses on semantic navigation tasks rather than state-coverage efficiency. Our proposal uniquely integrates meta-RL to learn transferable probing strategies across apps, reducing query budgets on unseen apps.",
        "Abstract": "Reconstructing the complete UI state graph of a mobile application via black-box exploration underpins testing, accessibility, and automated code synthesis. Existing methods require hundreds of probes per app and fail to capitalize on shared exploration structure across applications. We introduce MetaUIExplore, the first framework that applies meta-reinforcement learning to black-box UI exploration. During meta-training, we deploy a probing agent across N diverse Android apps, optimizing for rapid state-coverage with few interactions. We adopt a Model-Agnostic Meta-Learning (MAML) setup where the agent learns a shared initialization of its policy network, which can be fine-tuned on a new app using just K initial probes. At meta-test time, MetaUIExplore adapts in K=20\u201350 interactions and then continues exploration, achieving 80% state coverage with 30\u201350% fewer probes than non-meta baselines (UIProbe, random, BFS). We demonstrate on 15 open-source apps that MetaUIExplore consistently accelerates state graph recovery, particularly for apps with >20 screens. Finally, we show that downstream test-case generation from meta-extracted graphs retains high fault-detection rates while lowering exploration costs. MetaUIExplore reveals that reusable exploration priors can transform black-box UI analysis in resource-constrained settings.",
        "Experiments": [
            "Meta-Training & Adaptation: Meta-train on 12 Android apps (F-Droid). At meta-test, adapt on 3 held-out apps for K={10,20,50} initial probes. Measure probes needed for 80% coverage vs. UIProbe and BFS.",
            "Ablation (Meta vs. Non-Meta): Compare MAML-based policy to: (a) policy trained from scratch per-app, (b) random initialization, (c) behavior-cloning from traces. Report sample efficiency gains.",
            "Impact of Support Set Size: Vary K in {5,10,20,50}. Plot coverage vs. probes to quantify adaptation speed.",
            "Generalization to New Categories: Evaluate on apps from unseen categories (e.g., finance) to test transfer limits; report coverage ratio vs. category distance.",
            "Downstream Test Generation: Using reconstructed graphs from each method, generate path-based tests and compare fault-detection per 100 probes."
        ],
        "Risk Factors and Limitations": [
            "Meta-RL can overfit to the training-app distribution and fail on highly novel UIs.",
            "High computational cost for meta-training across many apps may require GPU clusters.",
            "Choice of K trades off between adaptation speed and initialization quality; too small K may under-adapt.",
            "Complex gestures (long-press, multi-touch) are not covered in our simple action set.",
            "State clustering errors propagate; meta-learned policies may amplify initial misclusterings."
        ]
    },
    {
        "Name": "uirevise_regression",
        "Title": "UIRevise: Automated Cross-Version UI Regression Test Generation via State Graph Differencing",
        "Short Hypothesis": "We hypothesize that by automatically extracting and diffing UI state graphs from two successive versions of a mobile app, we can generate focused regression test cases that detect UI-driven breaks with higher precision and lower effort than unguided or replay-based testing. This cross-version graph differencing isolates changed states and transitions to prioritize tests on modified UI behaviors without requiring manual specification.",
        "Related Work": "Prior GUI regression approaches (e.g., capture\u2010and\u2010replay tools, image\u2010based diffing) either require manual recording or suffer from noise and brittleness. GUI ripping and state\u2010graph reconstruction methods build abstract models of UI workflows but stop short of regression testing. Differential testing of apps has focused on API or functional behavior, not UI state flows. To our knowledge, no prior work synthesizes regression tests by differencing black-box UI state graphs across app versions.",
        "Abstract": "Maintaining UI consistency across app versions is laborious: small layout or workflow changes can break user flows or introduce regressions unnoticed by manual testing. We present UIRevise, a novel framework that automatically generates UI regression tests by differencing reconstructed UI state graphs of two app versions. UIRevise first applies black-box exploration to each version via lightweight probing and screenshot clustering to produce state graphs. It then performs graph differencing to identify new, removed, or modified states and transitions. For each affected edge, UIRevise synthesizes a concrete probe sequence that exercises the altered UI behavior, yielding a targeted regression test suite. We implement UIRevise atop Android emulators without requiring source access. In experiments on five open-source Android apps with known UI regressions across versions, UIRevise detects 92% of injected UI breaks using on average 25% of the probes needed by random exploration and 40% fewer tests than replay\u2010based methods. A small developer study shows that UIRevise tests are concise, robust to minor style changes, and focus developer attention on true regressions. UIRevise demonstrates that cross\u2010version UI graph differencing is an effective, automated path to regression testing for GUI\u2010centric applications.",
        "Experiments": [
            "State Graph Extraction: For each of five open\u2010source Android apps and two successive releases with known UI regressions, run unguided probing (e.g., random taps) to build state graphs per version; measure coverage and graph size.",
            "Graph Differencing: Compute diff on state graphs to identify changed states/transitions; evaluate precision by comparing against manually annotated UI changes in release notes.",
            "Regression Test Synthesis: For each diff edge, synthesize tap sequences via shortest path over combined graph. Measure test count and probe count versus random exploration baseline and record\u2010and\u2010replay of manual tests.",
            "Regression Detection: Seed UI regressions (e.g., broken navigation, removed buttons) in target apps. Run synthesized tests and baselines; compute detection rate and time to first failure.",
            "Developer Evaluation: Present generated regression tests to 5 mobile developers; survey readability (1\u20135), perceived relevance, and time to understand tests compared to manual scripts."
        ],
        "Risk Factors and Limitations": [
            "Graph extraction may miss dynamic screens driven by external data or complex gestures, weakening diff accuracy.",
            "Visual clustering may conflate visually similar but semantically distinct screens, yielding false positive or negative diffs.",
            "Mapping diff regions to concrete input sequences may fail when UI element positions change drastically between versions.",
            "Framework relies on emulator probing; network\u2010dependent or sensor\u2010driven UIs may not be reachable via simple taps.",
            "Highly frequent minor style tweaks (e.g., color changes) may generate spurious regression tests unless filtered by semantic differencing."
        ]
    },
    {
        "Name": "magnetoprobe_state_graph",
        "Title": "MagnetoGraph: Black-Box UI State Graph Reconstruction via Magnetic Side-Channel Probing",
        "Short Hypothesis": "We hypothesize that screen content induces distinctive magnetic side-channel signatures measurable by a device\u2019s magnetometer, and that by actively probing an unknown app\u2019s UI and clustering these signatures, we can reconstruct its state-transition graph without screenshots or instrumentation. This isolates and leverages magnetometer readings to distinguish UI states with minimal interactions, offering an alternative black-box exploration paradigm.",
        "Related Work": "Side-channel app fingerprinting studies (MagneticSpy, DeepMag+) leverage magnetometer readings to identify apps or webpage loads but focus on single-screen classification. Stylus eavesdropping attacks (iStelan, S3) exploit magnetometer for handwriting recognition. Black-box UI state reconstruction (UIProbe) uses screenshots and vision-language models. To our knowledge, no prior work uses magnetometer side-channels to reconstruct full UI state graphs, making our proposal both novel and non-trivial.",
        "Abstract": "Reconstructing a mobile application's UI state graph under black-box constraints is vital for automated testing, accessibility, and program synthesis. Existing approaches rely on visual screenshots with embedded vision-language models or on power and acoustic telemetry, requiring instrumentation or screen access. We introduce MagnetoGraph, a fully black-box UI exploration framework that exploits magnetic side-channels: as screen content changes, pixel-level color and brightness fluctuations alter the device\u2019s electromagnetic emissions, measurable by its onboard magnetometer. MagnetoGraph issues active probes (taps, swipes) to navigate, records magnetometer time-series at each interaction, and encodes them into a learned signature space. By clustering these signatures to identify unique states and building state transitions from probe actions, MagnetoGraph recovers a near-complete UI state graph without screenshots or OS instrumentation. We implement MagnetoGraph on commodity Android phones and evaluate on six real-world apps. Compared to screenshot-based crawling, MagnetoGraph achieves 85% of state coverage with 40% fewer interactions and discovers 20% more rare states under fixed budgets. We also demonstrate downstream test-case generation, achieving comparable fault detection rates. Our approach is lightweight, generalizes across devices and UI styles, and opens a new magnetic side-channel paradigm for UI analysis.",
        "Experiments": [
            "Side-Channel Signature Quality Study: Record magnetometer traces for a held-out app with labeled screens under static taps. Cluster traces and measure purity and Adjusted Rand Index vs ground-truth states.",
            "Graph Reconstruction Benchmark: Evaluate MagnetoGraph on six Android apps, compare interactions to achieve 80% and 90% state coverage against UIProbe (screenshot-based) and random crawlers.",
            "Acquisition Policy Ablation: Compare active uncertainty-based probe selection vs random action selection; report interactions to cover 80% states.",
            "Multi-Modal Fusion Study: Combine magnetometer with power side-channel signals to assess joint clustering quality and coverage gains.",
            "Downstream Test-Case Generation: Generate path-based test cases from reconstructed graphs; compare fault detection rates per interaction to baselines."
        ],
        "Risk Factors and Limitations": [
            "Magnetometer signals may be noisy or masked by environmental magnetic interference (speakers, motors), reducing signature distinctiveness.",
            "Some UI screens with similar color distributions may yield indistinguishable signatures, causing state conflation.",
            "Probe actions rely on consistent magnetic response; dynamic content (animations, video) could introduce temporal variability.",
            "Hardware variability across devices leads to different sensitivity; may require per-device calibration.",
            "Extreme low-light or screen brightness settings may weaken magnetic emissions, limiting applicability."
        ]
    }
]